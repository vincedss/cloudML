{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing correlations betweeen variables in loans_application_history\n",
    "\n",
    "In this notebook, we are going to study the correlations.\n",
    "\n",
    "* [Setup and loading the data](#setup)\n",
    "* [Correlation matrix](#corr-matrix)\n",
    "* [Scatterplot matrix](#scatter-matrix)\n",
    "* [Detailed analysis between two variables](#two-vars)\n",
    "\n",
    "<center><strong>Select Cell > Run All to execute the whole analysis</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dataset loading <a id=\"setup\" /> \n",
    "\n",
    "First of all, let's load the libraries that we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import dataiku                          # Access to Dataiku datasets\n",
    "import pandas as pd, numpy as np        # Data manipulation \n",
    "import scipy.cluster.hierarchy as sch   # Used for reordering the correlation matrix\n",
    "import seaborn as sns                   # Graphing\n",
    "sns.set(style=\"white\")                  # Tuning the style of charts\n",
    "import warnings                         # Disable some warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is now to load the dataset and put aside the three main types of columns:\n",
    "\n",
    "* Numerics\n",
    "* Categorical\n",
    "* Dates\n",
    "\n",
    "Since analyzing correlations requires to have the data in memory, we are only going to load a sample of the data. Modify the following cell to change the size of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a handle on the dataset\n",
    "mydataset = dataiku.Dataset(\"loans_application_history\")\n",
    "\n",
    "# Load the first 100'000 lines.\n",
    "# You can also load random samples, limit yourself to some columns, or only load\n",
    "# data matching some filters.\n",
    "#\n",
    "# Please refer to the Dataiku Python API documentation for more information\n",
    "df = mydataset.get_dataframe(\n",
    "    limit = 100000)\n",
    "\n",
    "# Get the column names\n",
    "numerical_columns = list(df.select_dtypes(include=[np.number]).columns)\n",
    "categorical_columns = list(df.select_dtypes(include=[object]).columns)\n",
    "date_columns = list(df.select_dtypes(include=['<M8[ns]']).columns)\n",
    "\n",
    "# Print a quick summary of what we just loaded\n",
    "print \"Loaded dataset\"\n",
    "print \"   Rows: %s\" % df.shape[0]\n",
    "print \"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
    "                                                    len(numerical_columns), len(categorical_columns),\n",
    "                                                    len(date_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation matrix <a id=\"corr-matrix\" />\n",
    "\n",
    "The very first correlation analysis consists of plotting the \"Correlation matrix\" for numerical variables.\n",
    "\n",
    "For each couple of numerical variables, this computes the \"strength\" of the correlation (called the Pearson coefficient):\n",
    "\n",
    " * 1.0 means a perfect correlation\n",
    " * 0.0 means no correlation\n",
    " * -1.0 means a perfect \"inverse\" correlation\n",
    " \n",
    "Since it does not really make sense to print this correlation plot for hundred of variables, we are restricting it to the first 50 numerical variables of the dataset. Modify the following cell to change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables to plot for the correlation matrix\n",
    "corr_matrix_vars = numerical_columns[0:50]\n",
    "\n",
    "print \"Plotting the correlation matrix on the following variables : %s\" % corr_matrix_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the requested columns\n",
    "df_corr_matrix = df[corr_matrix_vars]\n",
    "\n",
    "# This computes the Pearson coefficient for all couples\n",
    "corr = df_corr_matrix.corr().fillna(0)\n",
    "\n",
    "# Start drawing\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "size = max(10, len(corr.columns)/2.)\n",
    "f, ax = plt.subplots(figsize=(size, size))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, square=True, linewidths=.5, cbar_kws={\"shrink\": 0.5}, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reordered correlation matrix\n",
    "\n",
    "An interesting improvement over the correlation matrix is to reorder it by similarity between the variables so that the \"groups\" of variables that are strongly correlated appear close in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features and distance matrix.\n",
    "D = corr.values\n",
    "# Compute and plot dendrogram.\n",
    "Y = sch.linkage(D, method='centroid')\n",
    "Z = sch.dendrogram(Y, orientation='right',no_plot=True)\n",
    "# Compute distance matrix.\n",
    "index = Z['leaves']\n",
    "D = D[index,:]\n",
    "D = D[:,index]\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "size = max(10, len(corr.columns)/2.)\n",
    "f, ax = plt.subplots(figsize=(size, size))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "\n",
    "sns.heatmap(D, mask=mask, square=True, linewidths=.5, cbar_kws={\"shrink\": 0.5}, ax=ax)\n",
    "#ax.set(xticks=range(len(corr.columns)), xticklabels=corr.columns[index], yticks=range(len(corr.columns)), yticklabels=reversed(corr.columns[index]))\n",
    "ax.set_xticklabels(corr.columns[index], rotation=90, ha='center');\n",
    "ax.set_yticklabels(reversed(corr.columns[index]), rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter matrices <a id=\"scatter-matrix\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only generate the scatterplot matrix on a sample\n",
    "df_scatter_samp = df.sample(min(5000, df.shape[0])) # 5000 points maximum on the scatter plot\n",
    "\n",
    "# Take the first 4 numerical variables to plot the scatterplot matrix\n",
    "scatter_matrix_vars = numerical_columns[0:4]\n",
    "\n",
    "# If we have categorical variables, use the categorical variables with the lowest number of modalities\n",
    "# to plot the points of the scatterplot\n",
    "scatter_matrix_color = None\n",
    "\n",
    "cat_cols_with_cards = [(x, df[x].nunique()) for x in categorical_columns]\n",
    "# We don't want to take a column with only a single modality\n",
    "# and also we don't want variables with more than 10 modalities (would not really make sense to plot)\n",
    "cat_cols_with_cards_f = [x for x in cat_cols_with_cards if x[1] >= 2 and x[1] <= 10]\n",
    "\n",
    "if len(cat_cols_with_cards_f) > 0:\n",
    "    # We have at least one categorical variable with a good number of modalities, use it\n",
    "    scatter_matrix_color = sorted(cat_cols_with_cards_f, key= lambda c : c[1])[0][0]\n",
    "    \n",
    "print \"We will plot the following numerical variables : %s\" % scatter_matrix_vars\n",
    "if scatter_matrix_color is not None:\n",
    "    print \"Coloring the scatters by: %s\" % scatter_matrix_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you want to take manual control over which variables are plotted\n",
    "# scatter_matrix_vars = [\"num1\", \"num2\", \"num3\"]\n",
    "# scatter_matrix_color = \"cat1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn (the graphic library) doesn't like NaNs, so fill the matrix\n",
    "df_filled = df.fillna(0)\n",
    "sns.pairplot(df_filled, vars = scatter_matrix_vars, hue=scatter_matrix_color, size=3, palette=\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the relation between two features, including categorical features <a id=\"two-vars\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cardinalities of our categorical variables\n",
    "cat_cols_with_cards = [(x, df[x].nunique()) for x in categorical_columns]\n",
    "\n",
    "# For proper display, we only want columns with modalities between 2 and 10\n",
    "cat_cols_with_cards_f = [x for x in cat_cols_with_cards if x[1] >= 2 and x[1] <= 10]\n",
    "\n",
    "nb_suitable_cats = len(cat_cols_with_cards_f)\n",
    "nb_num = len(numerical_columns)\n",
    "\n",
    "if nb_suitable_cats >= 1 and nb_num >= 1:\n",
    "    tf_feat1 = cat_cols_with_cards_f[0][0]\n",
    "    tf_feat2 = numerical_columns[0]\n",
    "\n",
    "elif nb_suitable_cats == 0 and nb_num >= 2:\n",
    "    tf_feat1 = numerical_columns[0]\n",
    "    tf_feat2 = numerical_columns[1]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Failed to automatically select proper variables to plot, please select manually\")\n",
    "\n",
    "print \"Will plot relation between these two features: '%s' and '%s'\" % (tf_feat1, tf_feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment this to take control on the two variables\n",
    "# tf_feat1 = \"my_feat_1\"\n",
    "# tf_feat2 = \"my_feat_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf_feat1 in numerical_columns and tf_feat2 in numerical_columns:\n",
    "    sns.pairplot(df[[tf_feat1, tf_feat2]])\n",
    "    \n",
    "if tf_feat1 in numerical_columns and tf_feat2 in categorical_columns:\n",
    "    sns.FacetGrid(df, col=tf_feat2, col_wrap=5, hue=None).map(sns.distplot, tf_feat1)\n",
    "    \n",
    "if tf_feat1 in categorical_columns and tf_feat2 in numerical_columns:\n",
    "    sns.FacetGrid(df, col=tf_feat1, col_wrap=5, hue=None).map(sns.distplot, tf_feat2)\n",
    "    \n",
    "if tf_feat1 in categorical_columns and tf_feat2 in categorical_columns:\n",
    "    tf_list = [tf_feat1, tf_feat2]\n",
    "    tf_unique_count = [df[feat].unique().__len__() for feat in tf_list]\n",
    "    tf_min_loc = tf_unique_count.index(min(tf_unique_count))\n",
    "    sns.FacetGrid(data=df, col=tf_list[tf_min_loc], col_wrap=5, hue=None).map(sns.countplot, tf_list[(tf_min_loc+1)%2], order=df[tf_list[(tf_min_loc+1)%2]].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "analyzedDataset": "loans_application_history",
  "creator": "vdestoecklin",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
